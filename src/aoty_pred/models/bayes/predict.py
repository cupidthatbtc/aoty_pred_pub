"""Posterior predictive sampling and prediction for new artists.

This module provides prediction functionality for fitted NumPyro models:
- generate_posterior_predictive: Generate predictions for existing data
- predict_out_of_sample: Predict on held-out data with known artists
- predict_new_artist: Predict for unseen artists using population distribution

Key concepts:
- Posterior predictive samples are generated by running the model with
  posterior parameter samples and y=None (to generate predictions instead
  of conditioning on observed values)
- For new artists not seen during training, we sample from the population
  distribution Normal(mu_artist, sigma_artist) to get artist effects that
  incorporate uncertainty about the new artist's quality level
"""

from dataclasses import dataclass
from typing import Callable

import jax.numpy as jnp
from jax import random
from numpyro.infer import MCMC, Predictive

from aoty_pred.models.bayes.model import compute_sigma_scaled

__all__ = [
    "PredictionResult",
    "generate_posterior_predictive",
    "predict_out_of_sample",
    "predict_new_artist",
]


@dataclass
class PredictionResult:
    """Container for posterior predictive results.

    This is not frozen because it contains large mutable JAX arrays.

    Attributes:
        y: Predicted scores with observation noise.
            Shape: (n_samples, n_obs) where n_samples = num_chains * num_samples
        mu: Mean predictions without observation noise (optional).
            Shape: (n_samples, n_obs) if computed, otherwise None.
            Useful for separating epistemic uncertainty (spread of mu)
            from aleatoric uncertainty (sigma_obs).
    """

    y: jnp.ndarray
    mu: jnp.ndarray | None = None


def generate_posterior_predictive(
    model: Callable,
    mcmc: MCMC,
    model_args: dict,
    seed: int = 1,
    return_mu: bool = False,
) -> PredictionResult:
    """Generate posterior predictive samples for existing data.

    This function generates predictions using the fitted model's posterior
    samples. It's useful for:
    - Posterior predictive checks (comparing predictions to observed data)
    - Computing residuals and model fit diagnostics
    - Generating predictions on the training data

    Parameters
    ----------
    model : Callable
        NumPyro model function used for fitting (e.g., user_score_model).
    mcmc : MCMC
        Fitted MCMC object with posterior samples.
    model_args : dict
        Arguments to pass to the model. Must include the same keys as
        used during fitting: artist_idx, album_seq, prev_score, X, y,
        n_artists, max_seq. The 'y' value will be set to None internally.
    seed : int, default 1
        Random seed for reproducibility.
    return_mu : bool, default False
        If True, also extract mean predictions (mu) from the model.
        Note: mu extraction requires the model to record a 'mu' deterministic,
        which may not be available for all models.

    Returns
    -------
    PredictionResult
        Container with predictions:
        - y: Shape (n_samples, n_obs) posterior predictive samples
        - mu: Shape (n_samples, n_obs) if return_mu=True and available

    Example
    -------
    >>> from aoty_pred.models.bayes import user_score_model, fit_model
    >>> result = fit_model(user_score_model, model_args)
    >>> ppc = generate_posterior_predictive(user_score_model, result.mcmc, model_args)
    >>> print(f"PPC shape: {ppc.y.shape}")  # (n_samples, n_obs)

    Notes
    -----
    CRITICAL: The function sets y=None in model_args to generate predictions.
    This tells NumPyro to sample from the likelihood rather than condition
    on observed values.

    The batch_ndims=1 setting is used because mcmc.get_samples() returns
    flattened samples with shape (num_chains * num_samples, ...).
    """
    # Get posterior samples from MCMC
    posterior_samples = mcmc.get_samples()

    # Create Predictive with batch_ndims=1 (samples are flattened across chains)
    predictive = Predictive(
        model,
        posterior_samples=posterior_samples,
        batch_ndims=1,
    )

    # CRITICAL: Set y=None for prediction (don't condition on observed)
    pred_args = {**model_args, "y": None}

    # Generate predictions using modern JAX random API
    rng_key = random.key(seed)
    predictions = predictive(rng_key, **pred_args)

    # Extract predictions for the appropriate score type
    # Model uses prefixed names like "user_y" or "critic_y"
    y_key = None
    mu_key = None
    for key in predictions.keys():
        if key.endswith("_y"):
            y_key = key
        if key.endswith("_mu"):
            mu_key = key

    if y_key is None:
        # Fallback: try without prefix
        y_key = "y" if "y" in predictions else list(predictions.keys())[0]

    y_pred = predictions[y_key]

    # Extract mu if requested and available
    mu_pred = None
    if return_mu and mu_key is not None:
        mu_pred = predictions[mu_key]

    return PredictionResult(y=y_pred, mu=mu_pred)


def predict_out_of_sample(
    model: Callable,
    mcmc: MCMC,
    new_model_args: dict,
    seed: int = 2,
) -> PredictionResult:
    """Generate predictions for held-out data with known artists.

    This function is similar to generate_posterior_predictive but emphasizes
    that the data is new (e.g., validation or test set). It works for
    predicting albums from artists that were seen during training.

    Parameters
    ----------
    model : Callable
        NumPyro model function used for fitting (e.g., user_score_model).
    mcmc : MCMC
        Fitted MCMC object with posterior samples.
    new_model_args : dict
        Arguments for the new data. Must have same structure as training args:
        - artist_idx: Integer array mapping to artists seen during training
        - album_seq: Integer array with album sequence numbers
        - prev_score: Float array with previous album scores
        - X: Feature matrix
        - y: Should be None or the actual values for later comparison
        - n_artists: Same value used during training
        - max_seq: Maximum sequence number in this data
    seed : int, default 2
        Random seed for reproducibility.

    Returns
    -------
    PredictionResult
        Container with predictions for the new data.

    Example
    -------
    >>> # After fitting on training data
    >>> val_args = {
    ...     "artist_idx": val_artist_idx,  # Artists must be in training set
    ...     "album_seq": val_album_seq,
    ...     "prev_score": val_prev_score,
    ...     "X": val_X,
    ...     "y": None,
    ...     "n_artists": n_artists,  # Same as training
    ...     "max_seq": int(val_album_seq.max()),
    ... }
    >>> val_pred = predict_out_of_sample(user_score_model, result.mcmc, val_args)

    Warnings
    --------
    If artist_idx contains indices for artists NOT seen during training,
    the predictions will use the sampled artist effects at those indices,
    which may be undefined or lead to poor predictions. For truly unseen
    artists, use predict_new_artist instead.
    """
    # This is essentially the same as generate_posterior_predictive
    # but with a different default seed and documentation emphasis
    return generate_posterior_predictive(
        model=model,
        mcmc=mcmc,
        model_args=new_model_args,
        seed=seed,
        return_mu=False,
    )


def predict_new_artist(
    posterior_samples: dict,
    X_new: jnp.ndarray,
    prev_score: float | jnp.ndarray,
    n_reviews_new: jnp.ndarray | None = None,
    fixed_n_exponent: float | None = None,
    prefix: str = "user_",
    seed: int = 3,
    n_predictions: int | None = None,
) -> dict[str, jnp.ndarray]:
    """Predict for unseen artist using population distribution.

    For artists NOT seen during training, we cannot use their fitted artist
    effects. Instead, we sample a new artist effect from the population
    distribution Normal(mu_artist, sigma_artist), which incorporates
    uncertainty about the new artist's quality level.

    This naturally handles the hierarchical structure: a new artist is
    assumed to come from the same population as training artists, with
    additional uncertainty reflecting that we haven't observed them before.

    Parameters
    ----------
    posterior_samples : dict
        Dictionary of posterior samples from mcmc.get_samples().
        Must contain keys for:
        - {prefix}mu_artist: Population mean of artist effects
        - {prefix}sigma_artist: Between-artist standard deviation
        - {prefix}beta: Fixed effect coefficients
        - {prefix}rho: AR(1) coefficient
        - {prefix}sigma_obs: Observation noise
        - {prefix}n_exponent (optional): Heteroscedastic scaling exponent
    X_new : jnp.ndarray
        Feature vector for the new album. Shape: (n_features,) for single
        prediction or (n_albums, n_features) for multiple albums.
    prev_score : float or jnp.ndarray
        Previous album score for this artist:
        - Use 0.0 for debut albums (artist has no prior releases)
        - Use their last known score if available from external data
        - For multiple albums, provide array of shape (n_albums,)
    n_reviews_new : jnp.ndarray or None, default None
        Review counts for the new album(s). Required when the model uses
        heteroscedastic noise (either learned exponent in posterior or
        fixed_n_exponent provided). Shape: scalar for single album or
        (n_albums,) for multiple albums. Used to compute per-observation
        sigma_scaled.
    fixed_n_exponent : float or None, default None
        Fixed exponent for heteroscedastic noise scaling. Use this when the
        model was trained with a fixed non-zero n_exponent (i.e., the exponent
        is not in the posterior samples). When provided and non-zero,
        n_reviews_new must also be provided. Set to None or 0.0 for
        homoscedastic predictions.
    prefix : str, default "user_"
        Parameter name prefix ("user_" or "critic_" depending on model).
    seed : int, default 3
        Random seed for reproducibility.
    n_predictions : int or None, default None
        If specified, subsample the posterior to this many samples.
        Useful for reducing computation when full posterior isn't needed.

    Returns
    -------
    dict[str, jnp.ndarray]
        Dictionary containing:
        - "y": Full posterior predictive samples (with observation noise)
               Shape: (n_samples,) for single album, (n_samples, n_albums) for multiple
        - "mu": Mean predictions (without observation noise)
               Shape: same as y
        - "artist_effect": Sampled artist effects from population distribution
               Shape: (n_samples,)
        - "sigma_scaled": Per-observation noise scale used for predictions
               Shape: same as y. For homoscedastic models, this is sigma_obs
               broadcast to match y shape.

    Raises
    ------
    ValueError
        If heteroscedastic noise is active (learned exponent in posterior or
        non-zero fixed_n_exponent) but n_reviews_new is not provided.

    Example
    -------
    >>> posterior_samples = result.mcmc.get_samples()
    >>> X_new = jnp.array([0.5, -0.2, 0.1])  # Feature vector for new album
    >>>
    >>> # Debut album for new artist
    >>> pred = predict_new_artist(
    ...     posterior_samples, X_new, prev_score=0.0, prefix="user_"
    ... )
    >>> print(f"Mean prediction: {pred['mu'].mean():.1f}")
    >>> print(f"95% CI: [{jnp.percentile(pred['y'], 2.5):.1f}, "
    ...       f"{jnp.percentile(pred['y'], 97.5):.1f}]")

    Notes
    -----
    The prediction incorporates three sources of uncertainty:
    1. Parameter uncertainty (spread of beta, rho, etc. across samples)
    2. New artist uncertainty (sampled from population distribution)
    3. Observation noise (sigma_obs or sigma_scaled for heteroscedastic)

    For a debut album (prev_score=0.0), the AR(1) term contributes nothing,
    so the prediction is purely based on features and the population effect.

    Heteroscedastic noise: If the model was trained with learn_n_exponent=True,
    the posterior will contain {prefix}n_exponent samples. Alternatively, if
    the model was trained with a fixed non-zero exponent, pass fixed_n_exponent.
    In either case, n_reviews_new must be provided so that sigma_scaled can be
    computed per-observation as sigma_obs / n_reviews^exponent. This allows
    predictions to have appropriately calibrated uncertainty based on review count.
    """
    # Extract population parameters with prefix
    mu_artist = posterior_samples[f"{prefix}mu_artist"]
    sigma_artist = posterior_samples[f"{prefix}sigma_artist"]
    beta = posterior_samples[f"{prefix}beta"]
    rho = posterior_samples[f"{prefix}rho"]
    sigma_obs = posterior_samples[f"{prefix}sigma_obs"]

    n_samples = mu_artist.shape[0]

    # Check if model uses heteroscedastic noise (learned OR fixed exponent)
    has_learned_exponent = f"{prefix}n_exponent" in posterior_samples
    has_fixed_exponent = fixed_n_exponent is not None and fixed_n_exponent != 0
    has_n_exponent = has_learned_exponent or has_fixed_exponent

    # Validate n_reviews_new when required
    if has_n_exponent and n_reviews_new is None:
        mode = "learned" if has_learned_exponent else f"fixed (n_exponent={fixed_n_exponent})"
        raise ValueError(
            "n_reviews_new is required for predictions with heteroscedastic noise. "
            f"Model uses {mode} n_exponent but no n_reviews_new provided. "
            "Pass n_reviews_new as array of review counts (same length as X_new)."
        )

    # Optionally subsample the posterior
    if n_predictions is not None and n_predictions < n_samples:
        rng_key = random.key(seed)
        rng_key, subkey = random.split(rng_key)
        indices = random.choice(subkey, n_samples, shape=(n_predictions,), replace=False)
        mu_artist = mu_artist[indices]
        sigma_artist = sigma_artist[indices]
        beta = beta[indices]
        rho = rho[indices]
        sigma_obs = sigma_obs[indices]
        # Handle exponent samples based on learned vs fixed mode
        if has_learned_exponent:
            n_exponent_samples = posterior_samples[f"{prefix}n_exponent"]
            n_exponent_samples = n_exponent_samples[indices]
        elif has_fixed_exponent:
            # Create constant array matching subsampled sample count
            n_exponent_samples = jnp.full((n_predictions,), fixed_n_exponent)
        else:
            n_exponent_samples = None
        n_samples = n_predictions
        seed = seed + 1  # Use different key for subsequent sampling
    else:
        rng_key = random.key(seed)
        # Handle exponent samples based on learned vs fixed mode
        if has_learned_exponent:
            n_exponent_samples = posterior_samples[f"{prefix}n_exponent"]
        elif has_fixed_exponent:
            # Create constant array matching full sample count
            n_exponent_samples = jnp.full((n_samples,), fixed_n_exponent)
        else:
            n_exponent_samples = None

    # Handle X_new shape: ensure 2D for matmul
    X_new = jnp.atleast_1d(X_new)
    single_album = X_new.ndim == 1
    if single_album:
        X_new = X_new[None, :]  # Shape: (1, n_features)

    # Handle prev_score shape: ensure array
    prev_score = jnp.atleast_1d(jnp.asarray(prev_score))
    if single_album and prev_score.shape[0] == 1:
        prev_score = prev_score[0]  # Scalar for broadcasting

    # Handle n_reviews_new shape
    if n_reviews_new is not None:
        n_reviews_new = jnp.atleast_1d(jnp.asarray(n_reviews_new))
        if single_album and n_reviews_new.shape[0] == 1:
            n_reviews_new = n_reviews_new[0]  # Scalar for broadcasting

    # Sample new artist effect from population distribution
    rng_key, subkey = random.split(rng_key)
    new_artist_effect = mu_artist + sigma_artist * random.normal(subkey, (n_samples,))

    # Compute mean prediction: artist_effect + X @ beta + rho * prev_score
    # beta has shape (n_samples, n_features)
    # X_new has shape (n_albums, n_features)
    # Result: (n_samples, n_albums)
    linear_term = jnp.einsum("sf,af->sa", beta, X_new)  # (n_samples, n_albums)

    # AR term
    ar_term = rho[:, None] * prev_score  # (n_samples, 1) or (n_samples, n_albums)

    # Combine terms
    mu_pred = new_artist_effect[:, None] + linear_term + ar_term

    # Compute per-observation sigma for predictions
    # Note: At this point mu_pred has shape (n_samples, n_albums) even for single_album
    # (where n_albums=1). We maintain this 2D shape until the final squeeze.
    if n_reviews_new is not None and has_n_exponent:
        # Heteroscedastic mode: use per-sample exponent values (learned or fixed)
        # sigma_obs shape: (n_samples,), n_exponent shape: (n_samples,)
        # Output sigma_scaled shape: (n_samples, n_albums) to match mu_pred
        if single_album:
            # n_reviews_new is scalar, expand for broadcast
            sigma_scaled = compute_sigma_scaled(
                sigma_obs[:, None],  # (n_samples, 1)
                jnp.array([[n_reviews_new]]),  # (1, 1)
                n_exponent_samples[:, None],  # (n_samples, 1)
            )  # Result: (n_samples, 1)
        else:
            # Vectorized: broadcast across albums
            # Use broadcasting: (n_samples, 1) op (1, n_albums) -> (n_samples, n_albums)
            sigma_scaled = compute_sigma_scaled(
                sigma_obs[:, None],
                n_reviews_new[None, :],
                n_exponent_samples[:, None],
            )
    else:
        # Homoscedastic fallback: (n_samples, 1) or (n_samples, n_albums)
        sigma_scaled = sigma_obs[:, None]

    # Sample from observation distribution
    rng_key, subkey = random.split(rng_key)
    y_pred = mu_pred + sigma_scaled * random.normal(subkey, mu_pred.shape)

    # Squeeze if single album
    if single_album:
        mu_pred = mu_pred.squeeze(axis=1)
        y_pred = y_pred.squeeze(axis=1)
        sigma_scaled = sigma_scaled.squeeze(axis=1)

    return {
        "y": y_pred,
        "mu": mu_pred,
        "artist_effect": new_artist_effect,
        "sigma_scaled": sigma_scaled,
    }
